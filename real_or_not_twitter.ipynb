{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "real-or-not-twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1N_kV-HOJkDakWmNPG6l7gB2ZDNmmWfDh",
      "authorship_tag": "ABX9TyOQnRL0i6gKh23B8e/BSRFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiabe/coding-practice/blob/master/real_or_not_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUlEqVgnuWI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a66905db-e6f0-4020-f43f-9ec1b8c8aab3"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Project/real_or_not/train.csv\")\n",
        "print(df.head())\n",
        "print(f\"shape : {df.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id keyword  ...                                               text target\n",
            "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
            "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
            "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
            "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
            "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "shape : (7613, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4jqYOvaay7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "5f050437-0183-4a0a-fc09-2a3f0fc8734f"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.countplot(x=\"target\", data=df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOqhSMnwbKwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "df[\"kfold\"] = -1\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "y = df.target.values\n",
        "kf = model_selection.StratifiedKFold(n_splits=3)\n",
        "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "  df.loc[v_, 'kfold'] = f\n",
        "df.to_csv(\n",
        "    \"/content/drive/My Drive/Project/real_or_not/train_folds.csv\",\n",
        "    index=False\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDTgL2uxdh6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "337ce51d-858c-44a4-f084-416a36f68f89"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "\n",
        "def run(fold):\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/Project/real_or_not/train_folds.csv\")\n",
        "  train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "  valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  print(\"fitting tokenizer...\")\n",
        "\n",
        "  VOCAB_SIZE = 10000\n",
        "  EMBEDDING_DIM = 16\n",
        "  MAX_LEN = 128\n",
        "  TRUNCATING = \"post\"\n",
        "  OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      num_words=VOCAB_SIZE,\n",
        "      oov_token=OOV_TOKEN\n",
        "  )\n",
        "\n",
        "  tokenizer.fit_on_texts(df.text.values.tolist())\n",
        "  \n",
        "  xtrain = tokenizer.texts_to_sequences(train_df.text.values)\n",
        "  xvalid = tokenizer.texts_to_sequences(valid_df.text.values)\n",
        "\n",
        "  xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      xtrain,\n",
        "      maxlen=MAX_LEN,\n",
        "      truncating=TRUNCATING\n",
        "  )\n",
        "  xvalid = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      xvalid,\n",
        "      maxlen=MAX_LEN\n",
        "  )\n",
        "  \n",
        "  ytrain = train_df.target.values\n",
        "  yvalid = valid_df.target.values\n",
        "\n",
        "  print(\"loading model...\")\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(\n",
        "                                   VOCAB_SIZE,\n",
        "                                   EMBEDDING_DIM,\n",
        "                                   input_length=MAX_LEN\n",
        "                                   ),\n",
        "                               tf.keras.layers.Flatten(),\n",
        "                               tf.keras.layers.Dense(6, activation=\"relu\"),\n",
        "                               tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      loss=\"binary_crossentropy\",\n",
        "      optimizer=\"adam\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  print(\"training model...\")\n",
        "  \n",
        "  EPOCHS = 10\n",
        "\n",
        "  model.fit(\n",
        "      xtrain,\n",
        "      ytrain,\n",
        "      validation_data=(xvalid, yvalid),\n",
        "      verbose=1,\n",
        "      epochs=EPOCHS\n",
        "  )\n",
        "\n",
        "  valid_preds = model.predict(xvalid)\n",
        "  valid_preds = np.array(valid_preds) >= 0.5\n",
        "  accuracy = metrics.accuracy_score(yvalid, valid_preds)\n",
        "  print(f\"Accuracy Score = {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run(fold=0)\n",
        "  run(fold=1)\n",
        "  run(fold=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fitting tokenizer...\n",
            "loading model...\n",
            "training model\n",
            "Epoch 1/10\n",
            "159/159 [==============================] - 1s 5ms/step - loss: 0.6689 - accuracy: 0.5685 - val_loss: 0.6435 - val_accuracy: 0.5705\n",
            "Epoch 2/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.5979 - accuracy: 0.6934 - val_loss: 0.5883 - val_accuracy: 0.7080\n",
            "Epoch 3/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.5114 - accuracy: 0.8071 - val_loss: 0.5496 - val_accuracy: 0.7786\n",
            "Epoch 4/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.4293 - accuracy: 0.8705 - val_loss: 0.5342 - val_accuracy: 0.7841\n",
            "Epoch 5/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.3573 - accuracy: 0.9048 - val_loss: 0.5352 - val_accuracy: 0.7778\n",
            "Epoch 6/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.2360 - accuracy: 0.9434 - val_loss: 0.5321 - val_accuracy: 0.7734\n",
            "Epoch 7/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.1193 - accuracy: 0.9637 - val_loss: 0.5816 - val_accuracy: 0.7616\n",
            "Epoch 8/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0777 - accuracy: 0.9746 - val_loss: 0.6158 - val_accuracy: 0.7620\n",
            "Epoch 9/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0612 - accuracy: 0.9773 - val_loss: 0.6541 - val_accuracy: 0.7636\n",
            "Epoch 10/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0508 - accuracy: 0.9809 - val_loss: 0.6909 - val_accuracy: 0.7597\n",
            "Accuracy Score = 0.7596532702915682\n",
            "fitting tokenizer...\n",
            "loading model...\n",
            "training model\n",
            "Epoch 1/10\n",
            "159/159 [==============================] - 1s 5ms/step - loss: 0.6617 - accuracy: 0.5974 - val_loss: 0.6107 - val_accuracy: 0.6903\n",
            "Epoch 2/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.5160 - accuracy: 0.7762 - val_loss: 0.4937 - val_accuracy: 0.7746\n",
            "Epoch 3/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.3577 - accuracy: 0.8621 - val_loss: 0.4636 - val_accuracy: 0.7884\n",
            "Epoch 4/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.2460 - accuracy: 0.9103 - val_loss: 0.4753 - val_accuracy: 0.7971\n",
            "Epoch 5/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.1673 - accuracy: 0.9433 - val_loss: 0.5024 - val_accuracy: 0.7900\n",
            "Epoch 6/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.1132 - accuracy: 0.9689 - val_loss: 0.5402 - val_accuracy: 0.7845\n",
            "Epoch 7/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0817 - accuracy: 0.9756 - val_loss: 0.5833 - val_accuracy: 0.7943\n",
            "Epoch 8/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0633 - accuracy: 0.9817 - val_loss: 0.6152 - val_accuracy: 0.7904\n",
            "Epoch 9/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0513 - accuracy: 0.9842 - val_loss: 0.6450 - val_accuracy: 0.7864\n",
            "Epoch 10/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0434 - accuracy: 0.9858 - val_loss: 0.6736 - val_accuracy: 0.7857\n",
            "Accuracy Score = 0.7856579984239559\n",
            "fitting tokenizer...\n",
            "loading model...\n",
            "training model\n",
            "Epoch 1/10\n",
            "159/159 [==============================] - 1s 5ms/step - loss: 0.6731 - accuracy: 0.5690 - val_loss: 0.6441 - val_accuracy: 0.5704\n",
            "Epoch 2/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.5588 - accuracy: 0.7208 - val_loss: 0.5042 - val_accuracy: 0.7757\n",
            "Epoch 3/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.3789 - accuracy: 0.8481 - val_loss: 0.4565 - val_accuracy: 0.8029\n",
            "Epoch 4/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.2572 - accuracy: 0.9068 - val_loss: 0.4656 - val_accuracy: 0.8009\n",
            "Epoch 5/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.1753 - accuracy: 0.9417 - val_loss: 0.4768 - val_accuracy: 0.8002\n",
            "Epoch 6/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.1203 - accuracy: 0.9630 - val_loss: 0.5100 - val_accuracy: 0.7919\n",
            "Epoch 7/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0850 - accuracy: 0.9748 - val_loss: 0.5397 - val_accuracy: 0.7946\n",
            "Epoch 8/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0656 - accuracy: 0.9799 - val_loss: 0.5736 - val_accuracy: 0.7942\n",
            "Epoch 9/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0524 - accuracy: 0.9833 - val_loss: 0.6072 - val_accuracy: 0.7872\n",
            "Epoch 10/10\n",
            "159/159 [==============================] - 1s 4ms/step - loss: 0.0444 - accuracy: 0.9850 - val_loss: 0.6333 - val_accuracy: 0.7907\n",
            "Accuracy Score = 0.7906976744186046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMK08aiB1cSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "bb18ba16-8411-4201-cbcc-fa7aa5e2ca3c"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "a = [1, 2, 3, 4]\n",
        "len_a = len(a)\n",
        "\n",
        "batch_size = 10\n",
        "b = [0] * batch_size\n",
        "len_b = len(b)\n",
        "lines_idx = [*range(len_a)]\n",
        "print(f\"lines idx : {lines_idx}\")\n",
        "print()\n",
        "\n",
        "idx = 0\n",
        "batch_counter = 1\n",
        "random.shuffle(lines_idx)\n",
        "\n",
        "for i in range(len_b):\n",
        "  if idx >= len_a:\n",
        "    idx = 0\n",
        "    batch_counter += 1\n",
        "    random.shuffle(lines_idx)\n",
        "    print(\"idx >= len_a\")\n",
        "    print(f\"batch : {batch_counter}, b : {b}\")\n",
        "    print()\n",
        "    \n",
        "  b[i] = a[lines_idx[idx]]\n",
        "  idx += 1\n",
        "  print(\"idx < len_a\")\n",
        "  print(f\"batch : {batch_counter}, b : {b}\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines idx : [0, 1, 2, 3]\n",
            "\n",
            "idx < len_a\n",
            "batch : 1, b : [3, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 1, b : [3, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 1, b : [3, 1, 4, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 1, b : [3, 1, 4, 2, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "idx >= len_a\n",
            "batch : 2, b : [3, 1, 4, 2, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 2, b : [3, 1, 4, 2, 2, 0, 0, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 2, b : [3, 1, 4, 2, 2, 4, 0, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 2, b : [3, 1, 4, 2, 2, 4, 1, 0, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 2, b : [3, 1, 4, 2, 2, 4, 1, 3, 0, 0]\n",
            "\n",
            "idx >= len_a\n",
            "batch : 3, b : [3, 1, 4, 2, 2, 4, 1, 3, 0, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 3, b : [3, 1, 4, 2, 2, 4, 1, 3, 1, 0]\n",
            "\n",
            "idx < len_a\n",
            "batch : 3, b : [3, 1, 4, 2, 2, 4, 1, 3, 1, 3]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1L3pVoC6w5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U trax\n",
        "import trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa0TRFez6lhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1f994adc-8754-4875-83d8-9001e3b1546e"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Project/real_or_not/train.csv\")\n",
        "print(df.head())\n",
        "print(f\"shape : {df.shape}\")\n",
        "print(f\"dtype : {type(df)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id keyword  ...                                               text target\n",
            "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
            "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
            "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
            "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
            "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "shape : (7613, 5)\n",
            "dtype : <class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiJcPqwM6lre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b68f1084-b1bd-46bd-c4d3-c36147576756"
      },
      "source": [
        "def load_tweets():\n",
        "    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "    all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \n",
        "    return all_positive_tweets, all_negative_tweets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load positive and negative tweets\n",
        "all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "\n",
        "# View the total number of positive and negative tweets.\n",
        "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
        "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
        "\n",
        "# Split positive set into validation and training\n",
        "val_pos   = all_positive_tweets[3000:] # generating validation set for positive tweets\n",
        "train_pos  = all_positive_tweets[:3000]# generating training set for positive tweets\n",
        "\n",
        "# Split negative set into validation and training\n",
        "val_neg   = all_negative_tweets[3000:] # generating validation set for negative tweets\n",
        "train_neg  = all_negative_tweets[:3000] # generating training set for nagative tweets\n",
        "\n",
        "# Combine training data into one set\n",
        "train_x = train_pos + train_neg \n",
        "\n",
        "# Combine validation data into one set\n",
        "val_x  = val_pos + val_neg\n",
        "\n",
        "# Set the labels for the training set (1 for positive, 0 for negative)\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
        "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of positive tweets: 5000\n",
            "The number of negative tweets: 5000\n",
            "length of train_x 6000\n",
            "length of val_x 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLXpDFbt4XTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "03ac5c13-bef1-4f25-c52b-45dffd52afd8"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, twitter_samples \n",
        "\n",
        "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "# Stop words are messy and not that compelling; \n",
        "# \"very\" and \"not\" are considered stop words, but they are obviously expressing sentiment\n",
        "\n",
        "# The porter stemmer lemmatizes \"was\" to \"wa\".  Seriously???\n",
        "\n",
        "# I'm not sure we want to get into stop words\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "# Also have my doubts about stemming...\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \n",
        "    '''\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    ### START CODE HERE ###\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and # remove stopwords\n",
        "            word not in string.punctuation): # remove punctuation\n",
        "            #tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "    ### END CODE HERE ###\n",
        "    return tweets_clean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWz4LOhHSCd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e323614f-dc8f-4bac-d0d4-6349bf4700c3"
      },
      "source": [
        "\n",
        "\n",
        "# Include special tokens \n",
        "# started with pad, end of line and unk tokens\n",
        "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "\n",
        "# Note that we build vocab using training data\n",
        "for tweet in train_x: \n",
        "    processed_tweet = process_tweet(tweet)\n",
        "    for word in processed_tweet:\n",
        "        if word not in Vocab: \n",
        "            Vocab[word] = len(Vocab)\n",
        "    \n",
        "print(\"Total words in vocab are\",len(Vocab))\n",
        "#display(Vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in vocab are 7520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmSmzRRsegCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - A string containing a tweet\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "        \n",
        "    '''  \n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # Process the tweet into a list of words\n",
        "    # where only important words are kept (stop words removed)\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"List of words from the processed tweet:\")\n",
        "        print(word_l)\n",
        "        \n",
        "    # Initialize the list that will contain the unique integer IDs of each word\n",
        "    tensor_l = []\n",
        "    \n",
        "    # Get the unique integer ID of the __UNK__ token\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "        \n",
        "    # for each word in the list:\n",
        "    for word in word_l:\n",
        "        \n",
        "        # Get the unique integer ID.\n",
        "        # If the word doesn't exist in the vocab dictionary,\n",
        "        # use the unique ID for __UNK__ instead.\n",
        "        \n",
        "        word_ID = vocab_dict[word] if word in vocab_dict.keys() else unk_ID\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "        # Append the unique integer ID to the tensor list.\n",
        "        tensor_l.append(word_ID) \n",
        "    \n",
        "    return tensor_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZAnrZvH3sOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rnd\n",
        "\n",
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED: Data generator\n",
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "    '''\n",
        "    Input: \n",
        "        data_pos - Set of posstive examples\n",
        "        data_neg - Set of negative examples\n",
        "        batch_size - number of samples per batch. Must be even\n",
        "        loop - True or False\n",
        "        vocab_dict - The words dictionary\n",
        "        shuffle - Shuffle the data order\n",
        "    Yield:\n",
        "        inputs - Subset of positive and negative examples\n",
        "        targets - The corresponding labels for the subset\n",
        "        example_weights - An array specifying the importance of each example\n",
        "        \n",
        "    '''     \n",
        "### START GIVEN CODE ###\n",
        "    # make sure the batch size is an even number\n",
        "    # to allow an equal number of positive and negative samples\n",
        "    assert batch_size % 2 == 0\n",
        "    \n",
        "    # Number of positive examples in each batch is half of the batch size\n",
        "    # same with number of negative examples in each batch\n",
        "    n_to_take = batch_size // 2\n",
        "    \n",
        "    # Use pos_index to walk through the data_pos array\n",
        "    # same with neg_index and data_neg\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "    \n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "    \n",
        "    # Get and array with the data indexes\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "    \n",
        "    # shuffle lines if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "        \n",
        "    stop = False\n",
        "    \n",
        "    # Loop indefinitely\n",
        "    while not stop:  \n",
        "        \n",
        "        # create a batch with positive and negative examples\n",
        "        batch = []\n",
        "        \n",
        "        # First part: Pack n_to_take positive examples\n",
        "        \n",
        "        # Start from pos_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "                    \n",
        "            # If the positive index goes past the positive dataset lenght,\n",
        "            if pos_index >= len_data_pos: \n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                pos_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the positive sample\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "                    \n",
        "            # get the tweet as pos_index\n",
        "            tweet = data_pos[pos_index_lines[pos_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment pos_index by one\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "### END GIVEN CODE ###\n",
        "            \n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "        # Second part: Pack n_to_take negative examples\n",
        "    \n",
        "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "            \n",
        "            # If the negative index goes past the negative dataset length,\n",
        "            if neg_index >= len_data_neg:\n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                    \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                neg_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the negative sample\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "            # get the tweet as neg_index\n",
        "            tweet = data_neg[neg_index_lines[neg_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment neg_index by one\n",
        "            neg_index = neg_index + 1\n",
        "\n",
        "### END CODE HERE ###        \n",
        "\n",
        "### START GIVEN CODE ###\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        # Update the start index for positive data \n",
        "        # so that it's n_to_take positions after the current pos_index\n",
        "        pos_index += n_to_take\n",
        "        \n",
        "        # Update the start index for negative data \n",
        "        # so that it's n_to_take positions after the current neg_index\n",
        "        neg_index += n_to_take\n",
        "        \n",
        "        # Get the max tweet length (the length of the longest tweet) \n",
        "        # (you will pad all shorter tweets to have this length)\n",
        "        max_len = max([len(t) for t in batch]) \n",
        "        \n",
        "        \n",
        "        # Initialize the input_l, which will \n",
        "        # store the padded versions of the tensors\n",
        "        tensor_pad_l = []\n",
        "        \n",
        "        # Pad shorter tweets with zeros\n",
        "        for tensor in batch:\n",
        "            \n",
        "### END GIVEN CODE ###\n",
        "\n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
        "            n_pad = max_len - len(tensor)\n",
        "            \n",
        "            # Generate a list of zeros, with length n_pad\n",
        "            pad_l = [0] * n_pad\n",
        "            \n",
        "            # concatenate the tensor and the list of padded zeros\n",
        "            tensor_pad = tensor + pad_l\n",
        "            \n",
        "            # append the padded tensor to the list of padded tensors\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        # convert the list of padded tensors to a numpy array\n",
        "        # and store this as the model inputs\n",
        "        inputs = np.array(tensor_pad_l)\n",
        "  \n",
        "        # Generate the list of targets for the positive examples (a list of ones)\n",
        "        # The length is the number of positive examples in the batch\n",
        "        target_pos = [1] * n_to_take\n",
        "        \n",
        "        # Generate the list of targets for the negative examples (a list of zeros)\n",
        "        # The length is the number of negative examples in the batch\n",
        "        target_neg = [0] * n_to_take\n",
        "        \n",
        "        # Concatenate the positve and negative targets\n",
        "        target_l = target_pos + target_neg\n",
        "        \n",
        "        # Convert the target list into a numpy array\n",
        "        targets = np.array(target_l)\n",
        "\n",
        "        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
        "        example_weights = np.ones_like(targets)\n",
        "        \n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "### GIVEN CODE ###\n",
        "        # note we use yield and not return\n",
        "        yield inputs, targets, example_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-RoTRs1Lq3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "546984b7-033f-458b-89fa-3c6b9e27ef2e"
      },
      "source": [
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs: [[1506 1507  109   95   14 2551 2552    9]\n",
            " [ 991 1523   44    9  309  700   62    0]\n",
            " [3761   22  136 2057   50  136    0    0]\n",
            " [ 239   73 4023 6743 3761    0    0    0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M-diX0obwKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import trax\n",
        "\n",
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: classifier\n",
        "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
        "        \n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # create embedding layer\n",
        "    embed_layer = tl.Embedding(\n",
        "        vocab_size=vocab_size, # Size of the vocabulary\n",
        "        d_feature=embedding_dim)  # Embedding dimension\n",
        "    \n",
        "    # Create a mean layer, to create an \"average\" word embedding\n",
        "    mean_layer = tl.Mean(axis=1)\n",
        "    \n",
        "    # Create a dense layer, one unit for each output\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "\n",
        "    \n",
        "    # Create the log softmax layer (no parameters needed)\n",
        "    log_softmax_layer = tl.LogSoftmax()\n",
        "    \n",
        "    # Use tl.Serial to combine all layers\n",
        "    # and create the classifier\n",
        "    # of type trax.layers.combinators.Serial\n",
        "    model = tl.Serial(\n",
        "      embed_layer, # embedding layer\n",
        "      mean_layer, # mean layer\n",
        "      dense_output_layer, # dense output layer \n",
        "      log_softmax_layer # log softmax layer\n",
        "    )\n",
        "    \n",
        "### END CODE HERE ###     \n",
        "    \n",
        "    # return the model of type\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9pRukrej4kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "\n",
        "batch_size = 16\n",
        "rnd.seed(271)\n",
        "\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.01),\n",
        "    n_steps_per_checkpoint=10,\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")\n",
        "\n",
        "model = classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEceLY1JyEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93503bbc-447f-4ff6-ab0d-2be4a40ae46b"
      },
      "source": [
        "output_dir = '~/model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppQEb5YN0xnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_task, # The evaluation task\n",
        "                                output_dir) # The output directory\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "### END CODE HERE ###\n",
        "\n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYlAxdoY11c-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "6cb5ed00-c351-403f-ae88-7cfac1cd27e3"
      },
      "source": [
        "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-b0084f5b34fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_expand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-80-deaa29df631b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, train_task, eval_task, n_steps, output_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0mtrain_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The training task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                 \u001b[0meval_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The evaluation task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                 output_dir) # The output directory\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtraining_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/supervised/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tasks, eval_model, eval_tasks, output_dir, checkpoint_at, eval_at, n_devices, random_seed)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0meval_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_tasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Multitask training not supported yet.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0meval_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Multitask training not supported yet."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPMF55gZ2rz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}