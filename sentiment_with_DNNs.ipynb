{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-with-DNNs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPe7KSbufR7EEuNDCVuJSbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiabe/coding-practice/blob/master/sentiment_with_DNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00eVGS71Bi51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data set\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "#nltk.download('twitter_samples')\n",
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "#print(len(all_positive_tweets))\n",
        "#print(len(all_negative_tweets))\n",
        "\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "val_pos = all_positive_tweets[4000:]\n",
        "\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "val_neg = all_negative_tweets[4000:]\n",
        "\n",
        "#print(len(train_pos))\n",
        "#print(len(val_pos))\n",
        "#print(len(train_neg))\n",
        "#print(len(val_neg))\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "val_x = val_pos + val_neg\n",
        "\n",
        "#print(len(train_x))\n",
        "#print(len(val_x))\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "val_y = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "#print(len(train_y))\n",
        "#print(len(val_y))\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc66yqszTz2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process tweet\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  import re\n",
        "  import string\n",
        "\n",
        "  #nltk.download('stopwords')\n",
        "  from nltk.corpus import stopwords\n",
        "  from nltk.stem import PorterStemmer\n",
        "  from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, \n",
        "                             strip_handles=True, \n",
        "                             reduce_len=True)\n",
        "  \n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "  \n",
        "  tweets_clean = []\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)\n",
        "      tweets_clean.append(stem_word)\n",
        "  \n",
        "  return tweets_clean\n",
        "\n",
        "# test case\n",
        "#print(train_pos[0])\n",
        "#process_tweet(train_pos[0])\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHi3FWH0T5PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the vocabulary\n",
        "\n",
        "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}\n",
        "\n",
        "for tweet in train_x:\n",
        "  processed_tweet = process_tweet(tweet)\n",
        "  for word in processed_tweet:\n",
        "    if word not in Vocab:\n",
        "      Vocab[word] = len(Vocab)\n",
        "\n",
        "#print(len(Vocab))\n",
        "#Vocab\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cM-nlmGx0W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert tweet to tensor\n",
        "\n",
        "def tweet_to_tensor(tweet, vocab_dict, \n",
        "                    unk_token='__UNK__', verbose=False):\n",
        "  \n",
        "  word_l = process_tweet(tweet)\n",
        "\n",
        "  if verbose:\n",
        "    print(\"list of words from the processed tweet:\")\n",
        "    print(word_l)\n",
        "  \n",
        "  tensor_l = []\n",
        "  unk_ID = vocab_dict[unk_token]\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"the unique integer ID for the unk_token is {unk_ID}\")\n",
        "  \n",
        "  for word in word_l:\n",
        "    word_ID = vocab_dict[word] if word in vocab_dict.keys() else unk_ID\n",
        "    tensor_l.append(word_ID)\n",
        "  \n",
        "  return tensor_l\n",
        "\n",
        "# test case\n",
        "#print(val_pos[0])\n",
        "#tweet_to_tensor(val_pos[0], vocab_dict=Vocab, verbose=True)\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA-WYWNe5iSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f47fa9ff-73ad-475e-a65e-4f6e50e74923"
      },
      "source": [
        "# create batch generator\n",
        "\n",
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict,\n",
        "                   shuffle=False):\n",
        "  import random\n",
        "\n",
        "  assert batch_size % 2 == 0\n",
        "  \n",
        "  n_to_take = batch_size // 2\n",
        "\n",
        "  pos_index = 0\n",
        "  neg_index = 0\n",
        "\n",
        "  len_data_pos = len(data_pos)\n",
        "  len_data_neg = len(data_neg)\n",
        "\n",
        "  pos_index_lines = list(range(len_data_pos))\n",
        "  neg_index_lines = list(range(len_data_neg))\n",
        "\n",
        "  if shuffle:\n",
        "    random.shuffle(pos_index_lines)\n",
        "    random.shuffle(neg_index_lines)\n",
        "  \n",
        "  stop = False\n",
        "\n",
        "  while not stop:\n",
        "    batch = []\n",
        "    \n",
        "    for i in range(n_to_take):\n",
        "      if pos_index >= len_data_pos:\n",
        "        if not loop:\n",
        "          stop = True;\n",
        "          break;\n",
        "        \n",
        "        pos_index = 0\n",
        "\n",
        "        if shuffle:\n",
        "          random.shuffle(pos_index_lines)\n",
        "      \n",
        "      tweet = data_pos[pos_index_lines[pos_index]]\n",
        "      tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "      batch.append(tensor)\n",
        "      pos_index = pos_index + 1\n",
        "    \n",
        "    for i in range(n_to_take):\n",
        "      if neg_index >= len_data_neg:\n",
        "        if not loop:\n",
        "          stop = True;\n",
        "          break;\n",
        "        \n",
        "        neg_index = 0\n",
        "\n",
        "        if shuffle:\n",
        "          random.shuffle(neg_index_lines)\n",
        "      \n",
        "      tweet = data_neg[neg_index_lines[neg_index]]\n",
        "      tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "      batch.append(tensor)\n",
        "      neg_index = neg_index + 1\n",
        "    \n",
        "    if stop:\n",
        "      break;\n",
        "    \n",
        "    pos_index += n_to_take\n",
        "    neg_index += n_to_take\n",
        "\n",
        "    max_len = max([len(t) for t in batch])\n",
        "\n",
        "    tensor_pad_l = []\n",
        "\n",
        "    for tensor in batch:\n",
        "      n_pad = max_len - len(tensor)\n",
        "      pad_l = [0] * n_pad\n",
        "      tensor_pad = tensor + pad_l\n",
        "      tensor_pad_l.append(tensor_pad)\n",
        "    \n",
        "    inputs = np.array(tensor_pad_l)\n",
        "\n",
        "    target_pos = [1] * n_to_take\n",
        "    target_neg = [0] * n_to_take\n",
        "    target_l = target_pos + target_neg\n",
        "    targets = np.array(target_l)\n",
        "\n",
        "    example_weights = np.ones_like(targets)\n",
        "\n",
        "    yield inputs, targets, example_weights\n",
        "\n",
        "\n",
        "import random\n",
        "random.seed(30)\n",
        "\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
        "\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "print(f\"inputs : {inputs}\")\n",
        "print(f\"targets : {targets}\")\n",
        "print(f\"weight : {example_weights}\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs : [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
            " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
            " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
            " [ 250 3761    0    0    0    0    0    0    0    0    0]]\n",
            "targets : [1 1 0 0]\n",
            "weight : [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5WEoiWwT307",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "def classifier(vocab_size=len(vocab), embedding_dim=256, output_dim=2,\n",
        "               model='train'):\n",
        "  from trax import layers as tl\n",
        "\n",
        "  embed_layer = tl.Embedding(\n",
        "      vocab_size=vocab_size,\n",
        "      d_feature=embedding_dim)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}