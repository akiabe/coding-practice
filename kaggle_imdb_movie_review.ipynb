{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle-imdb-movie-review.ipynb",
      "provenance": [],
      "mount_file_id": "1SB81oQwLfcaC7kcAXP2rASixkmOyRcX1",
      "authorship_tag": "ABX9TyNL7dfXL2BpoHtMi+hqKw9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiabe/coding-practice/blob/master/kaggle_imdb_movie_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWr-OTo5GW88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "db1f16f9-e8b2-49ea-ab6a-1483d0d411a2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEVNHoMhGoCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5b1e835f-ed49-4e50-c047-7617329cec61"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Project/imdb_review/IMDB Dataset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FDb-wkfCtbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f2a0534a-955e-4f38-d56d-e515af094f09"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"../tmp/IMDB Dataset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJhd8LR4GdZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "44f1a9dc-cdaa-4a6d-ee05-b4ad092dd18d"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "sentence = \"hi, how are you?\"\n",
        "#sentence.split()\n",
        "word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', ',', 'how', 'are', 'you', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klcP3sGAGzaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "e2f88de8-0e88-4eef-f723-87d76d88dc60"
      },
      "source": [
        "# test (CountVectorizer)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "          \"hello, how are you?\",\n",
        "          \"im getting bored at home. And you? What do you think?\",\n",
        "          \"did you know about counts\",\n",
        "          \"let's see if this works!\",\n",
        "          \"YES!!!\"\n",
        "]\n",
        "\n",
        "ctv = CountVectorizer()\n",
        "ctv.fit(corpus)\n",
        "corpus_transformed = ctv.transform(corpus)\n",
        "print(corpus_transformed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 2)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 22)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 19)\t1\n",
            "  (1, 22)\t2\n",
            "  (2, 0)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 14)\t1\n",
            "  (2, 22)\t1\n",
            "  (3, 12)\t1\n",
            "  (3, 15)\t1\n",
            "  (3, 16)\t1\n",
            "  (3, 18)\t1\n",
            "  (3, 20)\t1\n",
            "  (4, 21)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUblSS8SlUH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f20ea40b-d28c-426f-f81d-f6bc1157f153"
      },
      "source": [
        "print(ctv.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAJYl-XOl51R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "28422f8f-2e5c-42a7-caff-5a28c28d6c8e"
      },
      "source": [
        "# test (CountVectorizer, word_tokenize)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "          \"hello, how are you?\",\n",
        "          \"im getting bored at home. And you? What do you think?\",\n",
        "          \"did you know about counts\",\n",
        "          \"let's see if this works!\",\n",
        "          \"YES!!!\"\n",
        "]\n",
        "\n",
        "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "ctv.fit(corpus)\n",
        "corpus_transformed = ctv.transform(corpus)\n",
        "print(ctv.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxSA9LbEqFgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "cbbf6d0e-f803-462a-919f-cada5827384b"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.countplot(x=\"sentiment\", data=df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFEZ7jNgrXmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "a4879326-f0a3-4106-b4a7-b5feadc6463e"
      },
      "source": [
        "# benchmark(logistic regression)\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  df = pd.read_csv(\"../tmp/IMDB Dataset.csv\")\n",
        "\n",
        "  df.sentiment = df.sentiment.apply(\n",
        "      lambda x: 1 if x == \"positive\" else 0\n",
        "  )\n",
        "\n",
        "  df[\"kfold\"] = -1\n",
        "  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  y = df.sentiment.values\n",
        "  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "  \n",
        "  for fold_ in range(5):\n",
        "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
        "\n",
        "    count_vec = CountVectorizer(\n",
        "        tokenizer=word_tokenize,\n",
        "        token_pattern=None\n",
        "    )\n",
        "\n",
        "    count_vec.fit(train_df.review)\n",
        "\n",
        "    xtrain = count_vec.transform(train_df.review)\n",
        "    xtest = count_vec.transform(test_df.review)\n",
        "\n",
        "    model = linear_model.LogisticRegression()\n",
        "\n",
        "    model.fit(xtrain, train_df.sentiment)\n",
        "\n",
        "    preds = model.predict(xtest)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
        "\n",
        "    print(f\"Fold : {fold_}\")\n",
        "    print(f\"Accuracy = {accuracy}\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 0\n",
            "Accuracy = 0.8902\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 1\n",
            "Accuracy = 0.8971\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 2\n",
            "Accuracy = 0.895\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 3\n",
            "Accuracy = 0.8892\n",
            "\n",
            "Fold : 4\n",
            "Accuracy = 0.8922\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlSEgg9M1Wcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "3fed3572-70be-4285-8dcc-0caf15dc0459"
      },
      "source": [
        "# other model (naive bayes)\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  df = pd.read_csv(\"../tmp/IMDB Dataset.csv\")\n",
        "\n",
        "  df.sentiment = df.sentiment.apply(\n",
        "      lambda x: 1 if x == \"positive\" else 0\n",
        "  )\n",
        "\n",
        "  df[\"kfold\"] = -1\n",
        "  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  y = df.sentiment.values\n",
        "  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "  \n",
        "  for fold_ in range(5):\n",
        "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
        "\n",
        "    count_vec = CountVectorizer(\n",
        "        tokenizer=word_tokenize,\n",
        "        token_pattern=None\n",
        "    )\n",
        "\n",
        "    count_vec.fit(train_df.review)\n",
        "\n",
        "    xtrain = count_vec.transform(train_df.review)\n",
        "    xtest = count_vec.transform(test_df.review)\n",
        "\n",
        "    model = naive_bayes.MultinomialNB()\n",
        "\n",
        "    model.fit(xtrain, train_df.sentiment)\n",
        "\n",
        "    preds = model.predict(xtest)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
        "\n",
        "    print(f\"Fold : {fold_}\")\n",
        "    print(f\"Accuracy = {accuracy}\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold : 0\n",
            "Accuracy = 0.8472\n",
            "\n",
            "Fold : 1\n",
            "Accuracy = 0.8409\n",
            "\n",
            "Fold : 2\n",
            "Accuracy = 0.8536\n",
            "\n",
            "Fold : 3\n",
            "Accuracy = 0.8399\n",
            "\n",
            "Fold : 4\n",
            "Accuracy = 0.8425\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz5J9EJy780I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "443e2e48-3e24-4f01-fc4c-da554856430a"
      },
      "source": [
        "# test (TfidfVectorizer)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "          \"hello, how are you?\",\n",
        "          \"im getting bored at home. And you? What do you think?\",\n",
        "          \"did you know about counts\",\n",
        "          \"let's see if this works!\",\n",
        "          \"YES!!!\"\n",
        "]\n",
        "\n",
        "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "tfv.fit(corpus)\n",
        "corpus_transformed = tfv.transform(corpus)\n",
        "print(corpus_transformed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 27)\t0.2965698850220162\n",
            "  (0, 16)\t0.4428321995085722\n",
            "  (0, 14)\t0.4428321995085722\n",
            "  (0, 7)\t0.4428321995085722\n",
            "  (0, 4)\t0.35727423026525224\n",
            "  (0, 2)\t0.4428321995085722\n",
            "  (1, 27)\t0.35299699146792735\n",
            "  (1, 24)\t0.2635440111190765\n",
            "  (1, 22)\t0.2635440111190765\n",
            "  (1, 18)\t0.2635440111190765\n",
            "  (1, 15)\t0.2635440111190765\n",
            "  (1, 13)\t0.2635440111190765\n",
            "  (1, 12)\t0.2635440111190765\n",
            "  (1, 9)\t0.2635440111190765\n",
            "  (1, 8)\t0.2635440111190765\n",
            "  (1, 6)\t0.2635440111190765\n",
            "  (1, 4)\t0.42525129752567803\n",
            "  (1, 3)\t0.2635440111190765\n",
            "  (2, 27)\t0.31752680284846835\n",
            "  (2, 19)\t0.4741246485558491\n",
            "  (2, 11)\t0.4741246485558491\n",
            "  (2, 10)\t0.4741246485558491\n",
            "  (2, 5)\t0.4741246485558491\n",
            "  (3, 25)\t0.38775666010579296\n",
            "  (3, 23)\t0.38775666010579296\n",
            "  (3, 21)\t0.38775666010579296\n",
            "  (3, 20)\t0.38775666010579296\n",
            "  (3, 17)\t0.38775666010579296\n",
            "  (3, 1)\t0.38775666010579296\n",
            "  (3, 0)\t0.3128396318588854\n",
            "  (4, 26)\t0.3818506296777547\n",
            "  (4, 0)\t0.9242240510908069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw3hDAPOASgT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e53415cf-6de6-4663-95f7-c0a4f6a37fea"
      },
      "source": [
        "print(tfv.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMR-VhzGAdm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "2e745ff2-27b9-4d22-91f2-1af80c5cb7de"
      },
      "source": [
        "# tfidf, Logistic Regression\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  df = pd.read_csv(\"../tmp/IMDB Dataset.csv\")\n",
        "\n",
        "  df.sentiment = df.sentiment.apply(\n",
        "      lambda x: 1 if x == \"positive\" else 0\n",
        "  )\n",
        "\n",
        "  df[\"kfold\"] = -1\n",
        "  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  y = df.sentiment.values\n",
        "  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "  \n",
        "  for fold_ in range(5):\n",
        "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
        "\n",
        "    tfidf_vec = TfidfVectorizer(\n",
        "        tokenizer=word_tokenize,\n",
        "        token_pattern=None\n",
        "    )\n",
        "\n",
        "    tfidf_vec.fit(train_df.review)\n",
        "\n",
        "    xtrain = tfidf_vec.transform(train_df.review)\n",
        "    xtest = tfidf_vec.transform(test_df.review)\n",
        "\n",
        "    model = linear_model.LogisticRegression()\n",
        "\n",
        "    model.fit(xtrain, train_df.sentiment)\n",
        "\n",
        "    preds = model.predict(xtest)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
        "\n",
        "    print(f\"Fold : {fold_}\")\n",
        "    print(f\"Accuracy = {accuracy}\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold : 0\n",
            "Accuracy = 0.9001\n",
            "\n",
            "Fold : 1\n",
            "Accuracy = 0.8999\n",
            "\n",
            "Fold : 2\n",
            "Accuracy = 0.8902\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 3\n",
            "Accuracy = 0.8982\n",
            "\n",
            "Fold : 4\n",
            "Accuracy = 0.8959\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIlDGkMoCVep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6c2eed4e-a670-4cda-8869-2eb5829c5144"
      },
      "source": [
        "# test (n-grams)\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "N = 3\n",
        "sentence = \"hi, how are you?\"\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "print(\"tokenized_sentence :\")\n",
        "print(tokenized_sentence)\n",
        "print()\n",
        "n_grams = list(ngrams(tokenized_sentence, N))\n",
        "print(\"n_grams :\")\n",
        "print(n_grams)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized_sentence :\n",
            "['hi', ',', 'how', 'are', 'you', '?']\n",
            "\n",
            "n_grams :\n",
            "[('hi', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7njXemaCVo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "c2838d44-eaaf-4a27-d0cc-b69d8af1b481"
      },
      "source": [
        "# tfidf, Logistic Regression\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  df = pd.read_csv(\"../tmp/IMDB Dataset.csv\")\n",
        "\n",
        "  df.sentiment = df.sentiment.apply(\n",
        "      lambda x: 1 if x == \"positive\" else 0\n",
        "  )\n",
        "\n",
        "  df[\"kfold\"] = -1\n",
        "  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  y = df.sentiment.values\n",
        "  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "  \n",
        "  for fold_ in range(5):\n",
        "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
        "\n",
        "    tfidf_vec = TfidfVectorizer(\n",
        "        tokenizer=word_tokenize,\n",
        "        token_pattern=None,\n",
        "        ngram_range=(1, 3)\n",
        "    )\n",
        "\n",
        "    tfidf_vec.fit(train_df.review)\n",
        "\n",
        "    xtrain = tfidf_vec.transform(train_df.review)\n",
        "    xtest = tfidf_vec.transform(test_df.review)\n",
        "\n",
        "    model = linear_model.LogisticRegression()\n",
        "\n",
        "    model.fit(xtrain, train_df.sentiment)\n",
        "\n",
        "    preds = model.predict(xtest)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
        "\n",
        "    print(f\"Fold : {fold_}\")\n",
        "    print(f\"Accuracy = {accuracy}\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 0\n",
            "Accuracy = 0.8946\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold : 1\n",
            "Accuracy = 0.8935\n",
            "\n",
            "Fold : 2\n",
            "Accuracy = 0.8923\n",
            "\n",
            "Fold : 3\n",
            "Accuracy = 0.8867\n",
            "\n",
            "Fold : 4\n",
            "Accuracy = 0.8933\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJd9PlVJCVtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4f5abd50-64c6-49da-fed8-3e1807697477"
      },
      "source": [
        "# test (kfold)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "for fold, (t_idx, v_idx) in enumerate(kf.split(x, y)):\n",
        "  print(fold)\n",
        "  print(t_idx)\n",
        "  print(v_idx)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[2 3 4 5 6 7 8 9]\n",
            "[0 1]\n",
            "1\n",
            "[0 1 4 5 6 7 8 9]\n",
            "[2 3]\n",
            "2\n",
            "[0 1 2 3 6 7 8 9]\n",
            "[4 5]\n",
            "3\n",
            "[0 1 2 3 4 5 8 9]\n",
            "[6 7]\n",
            "4\n",
            "[0 1 2 3 4 5 6 7]\n",
            "[8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-LMS0VCVw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "080f919a-87b1-4029-f6eb-491e237a573a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Project/imdb_review/IMDB Dataset.csv\")\n",
        "print(df.shape)\n",
        "\n",
        "df.sentiment = df.sentiment.apply(\n",
        "    lambda x: 1 if x == \"positive\" else 0\n",
        ")\n",
        "\n",
        "df[\"kfold\"] = -1  \n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "y = df.sentiment.values\n",
        "\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "  print(\"train_idx and kfold\")\n",
        "  print()\n",
        "  print(df.loc[t_, 'kfold'])\n",
        "  print()\n",
        "  print(\"val_idx and kfold\")\n",
        "  print()\n",
        "  print(df.loc[v_, 'kfold'])\n",
        "  print()\n",
        "  df.loc[v_, 'kfold'] = f\n",
        "  print(\"val_idx and replace kfold number\")\n",
        "  print()\n",
        "  print(df.loc[v_, 'kfold'])\n",
        "  print()\n",
        "\n",
        "#df.to_csv(\"/content/drive/My Drive/Project/imdb_review/train_folds.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n",
            "train_idx and kfold\n",
            "\n",
            "9941    -1\n",
            "9946    -1\n",
            "9947    -1\n",
            "9949    -1\n",
            "9952    -1\n",
            "        ..\n",
            "49995   -1\n",
            "49996   -1\n",
            "49997   -1\n",
            "49998   -1\n",
            "49999   -1\n",
            "Name: kfold, Length: 40000, dtype: int64\n",
            "\n",
            "val_idx and kfold\n",
            "\n",
            "0       -1\n",
            "1       -1\n",
            "2       -1\n",
            "3       -1\n",
            "4       -1\n",
            "        ..\n",
            "10039   -1\n",
            "10040   -1\n",
            "10041   -1\n",
            "10043   -1\n",
            "10044   -1\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "val_idx and replace kfold number\n",
            "\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "10039    0\n",
            "10040    0\n",
            "10041    0\n",
            "10043    0\n",
            "10044    0\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "train_idx and kfold\n",
            "\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "49995   -1\n",
            "49996   -1\n",
            "49997   -1\n",
            "49998   -1\n",
            "49999   -1\n",
            "Name: kfold, Length: 40000, dtype: int64\n",
            "\n",
            "val_idx and kfold\n",
            "\n",
            "9941    -1\n",
            "9946    -1\n",
            "9947    -1\n",
            "9949    -1\n",
            "9952    -1\n",
            "        ..\n",
            "20203   -1\n",
            "20204   -1\n",
            "20206   -1\n",
            "20207   -1\n",
            "20208   -1\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "val_idx and replace kfold number\n",
            "\n",
            "9941     1\n",
            "9946     1\n",
            "9947     1\n",
            "9949     1\n",
            "9952     1\n",
            "        ..\n",
            "20203    1\n",
            "20204    1\n",
            "20206    1\n",
            "20207    1\n",
            "20208    1\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "train_idx and kfold\n",
            "\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "49995   -1\n",
            "49996   -1\n",
            "49997   -1\n",
            "49998   -1\n",
            "49999   -1\n",
            "Name: kfold, Length: 40000, dtype: int64\n",
            "\n",
            "val_idx and kfold\n",
            "\n",
            "19779   -1\n",
            "19784   -1\n",
            "19785   -1\n",
            "19786   -1\n",
            "19788   -1\n",
            "        ..\n",
            "30145   -1\n",
            "30147   -1\n",
            "30148   -1\n",
            "30149   -1\n",
            "30151   -1\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "val_idx and replace kfold number\n",
            "\n",
            "19779    2\n",
            "19784    2\n",
            "19785    2\n",
            "19786    2\n",
            "19788    2\n",
            "        ..\n",
            "30145    2\n",
            "30147    2\n",
            "30148    2\n",
            "30149    2\n",
            "30151    2\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "train_idx and kfold\n",
            "\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "49995   -1\n",
            "49996   -1\n",
            "49997   -1\n",
            "49998   -1\n",
            "49999   -1\n",
            "Name: kfold, Length: 40000, dtype: int64\n",
            "\n",
            "val_idx and kfold\n",
            "\n",
            "29852   -1\n",
            "29854   -1\n",
            "29855   -1\n",
            "29856   -1\n",
            "29858   -1\n",
            "        ..\n",
            "40130   -1\n",
            "40133   -1\n",
            "40134   -1\n",
            "40136   -1\n",
            "40137   -1\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "val_idx and replace kfold number\n",
            "\n",
            "29852    3\n",
            "29854    3\n",
            "29855    3\n",
            "29856    3\n",
            "29858    3\n",
            "        ..\n",
            "40130    3\n",
            "40133    3\n",
            "40134    3\n",
            "40136    3\n",
            "40137    3\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "train_idx and kfold\n",
            "\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "40130    3\n",
            "40133    3\n",
            "40134    3\n",
            "40136    3\n",
            "40137    3\n",
            "Name: kfold, Length: 40000, dtype: int64\n",
            "\n",
            "val_idx and kfold\n",
            "\n",
            "39880   -1\n",
            "39883   -1\n",
            "39884   -1\n",
            "39885   -1\n",
            "39886   -1\n",
            "        ..\n",
            "49995   -1\n",
            "49996   -1\n",
            "49997   -1\n",
            "49998   -1\n",
            "49999   -1\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n",
            "val_idx and replace kfold number\n",
            "\n",
            "39880    4\n",
            "39883    4\n",
            "39884    4\n",
            "39885    4\n",
            "39886    4\n",
            "        ..\n",
            "49995    4\n",
            "49996    4\n",
            "49997    4\n",
            "49998    4\n",
            "49999    4\n",
            "Name: kfold, Length: 10000, dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feudihF2CV0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "55e44892-5ba1-4e2f-9743-e2ab65ac8bf3"
      },
      "source": [
        "# test (Snowball Stemmer and WordNet Lemmatizer)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "words = [\"fishing\", \"fishes\", \"fished\"]\n",
        "\n",
        "for word in words:\n",
        "  print(f\"word={word}\")\n",
        "  print(f\"stemmed_word={stemmer.stem(word)}\")\n",
        "  print(f\"lemma={lemmatizer.lemmatize(word)}\")\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "word=fishing\n",
            "stemmed_word=fish\n",
            "lemma=fishing\n",
            "\n",
            "word=fishes\n",
            "stemmed_word=fish\n",
            "lemma=fish\n",
            "\n",
            "word=fished\n",
            "stemmed_word=fish\n",
            "lemma=fished\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdttPzS0CV3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "6ec55db4-9694-44e5-ae38-5abe4858d5f9"
      },
      "source": [
        "# stemming or lemmatization and naive bayes\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/Project/imdb_review/IMDB Dataset.csv\")\n",
        "\n",
        "  df.sentiment = df.sentiment.apply(\n",
        "      lambda x: 1 if x == \"positive\" else 0\n",
        "  )\n",
        "\n",
        "  df[\"kfold\"] = -1\n",
        "\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  y = df.sentiment.values\n",
        "  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "  \n",
        "  for fold_ in range(5):\n",
        "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    #stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "    count_vec = CountVectorizer(\n",
        "        tokenizer=lemmatizer.lemmatize,\n",
        "        #tokenizer=stemmer.stem,\n",
        "        token_pattern=None\n",
        "    )\n",
        "\n",
        "    count_vec.fit(train_df.review)\n",
        "\n",
        "    xtrain = count_vec.transform(train_df.review)\n",
        "    xtest = count_vec.transform(test_df.review)\n",
        "\n",
        "    model = naive_bayes.MultinomialNB()\n",
        "\n",
        "    model.fit(xtrain, train_df.sentiment)\n",
        "\n",
        "    preds = model.predict(xtest)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
        "\n",
        "    print(f\"Fold : {fold_}\")\n",
        "    print(f\"Accuracy = {accuracy}\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold : 0\n",
            "Accuracy = 0.613\n",
            "\n",
            "Fold : 1\n",
            "Accuracy = 0.6151\n",
            "\n",
            "Fold : 2\n",
            "Accuracy = 0.6163\n",
            "\n",
            "Fold : 3\n",
            "Accuracy = 0.6254\n",
            "\n",
            "Fold : 4\n",
            "Accuracy = 0.6089\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwJNPLzECV6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7402df3d-953e-4abe-d0c4-d93901c279ce"
      },
      "source": [
        "# test (svd and TfidfVectorizer)\n",
        "import pandas as pd\n",
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import decomposition\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = pd.read_csv(\"/content/drive/My Drive/Project/imdb_review/IMDB Dataset.csv\", nrows=10000)\n",
        "corpus = corpus.review.values\n",
        "\n",
        "tfv = TfidfVectorizer(\n",
        "    tokenizer=word_tokenize,\n",
        "    token_pattern=None\n",
        ")\n",
        "\n",
        "tfv.fit(corpus)\n",
        "\n",
        "corpus_transformed = tfv.transform(corpus)\n",
        "svd = decomposition.TruncatedSVD(n_components=10)\n",
        "corpus_svd = svd.fit(corpus_transformed)\n",
        "\n",
        "sample_index = 0\n",
        "feature_scores = dict(\n",
        "    zip(\n",
        "        tfv.get_feature_names(),\n",
        "        corpus_svd.components_[sample_index]\n",
        "    )\n",
        ")\n",
        "\n",
        "N = 5\n",
        "print(\n",
        "    sorted(\n",
        "        feature_scores,\n",
        "        key=feature_scores.get,\n",
        "        reverse=True\n",
        "    )[:N]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', ',', '.', 'and', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BMTL-QzCV-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 5\n",
        "\n",
        "for sample_index in range(5):\n",
        "  feature_scores = dict(\n",
        "      zip(\n",
        "          tfv.get_feature_names(),\n",
        "          corpus_svd.components_[sample_index]\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      sorted(\n",
        "          feature_scores,\n",
        "          key=feature_scores.get,\n",
        "          reverse=True\n",
        "      )[:N]\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYFw9uVf8flM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b186baae-12ee-4d73-9a69-4927bf1a05bb"
      },
      "source": [
        "# test (clean text data)\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(s):\n",
        "  s = s.split()\n",
        "  s = \" \".join(s)\n",
        "  s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
        "  return s\n",
        "\n",
        "corpus = pd.read_csv(\"/content/drive/My Drive/Project/imdb_review/IMDB Dataset.csv\", nrows=10000)\n",
        "\n",
        "corpus.loc[:, \"review\"] = corpus.review.apply(clean_text)\n",
        "print(corpus.loc[:, \"review\"][:1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    One of the other reviewers has mentioned that ...\n",
            "Name: review, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVztTj7yJczi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
        "  words = str(s).lower()\n",
        "  words = tokenizer(words)\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  words = [w for w in words if w.isalpha()]\n",
        "\n",
        "  M = []\n",
        "  for w in words:\n",
        "    if w in embedding_dict:\n",
        "      M.append(embedding_dict[w])\n",
        "  \n",
        "  if len(M) == 0:\n",
        "    return np.zeros(300)\n",
        "  \n",
        "  M = np.array(M)\n",
        "  v = M.sum(axis=0)\n",
        "\n",
        "  return v / np.sqrt((v ** 2).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pJhCvLMPX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM with Pytorch\n",
        "import io\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "class IMDBDataset:\n",
        "  def __init__(self, reviews, targets):\n",
        "    self.reviews = reviews\n",
        "    self.target = targets\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = self.reviews[item, :]\n",
        "    target = self.target[item]\n",
        "\n",
        "    return{\n",
        "        \"review\": torch.tensor(review, dtype=torch.long),\n",
        "        \"target\": torch.tensor(target, dt)\n",
        "    }\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embedding_matrix):\n",
        "    super(LSTM, self).__init__()\n",
        "    num_words = embedding_matrix.shape[0]\n",
        "    embed_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    self.embedding = nn.Embedding(\n",
        "        num_embeddings=num_words,\n",
        "        embedding_dim=embed_dim\n",
        "    )\n",
        "\n",
        "    self.embedding.weight = nn.Parameter(\n",
        "        torch.tensor(\n",
        "            embedding_matrix,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "    )\n",
        "\n",
        "    self.embedding.weight.requires_grad = False\n",
        "\n",
        "    self.lstm = nn.LSTM(\n",
        "        embed_dim,\n",
        "        128,\n",
        "        bidirectional=True,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    self.out = nn.Linear(512, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x, _ = self.lstm(x)\n",
        "    avg_pool = torch.mean(x, 1)\n",
        "    max_pool, _ = torch.max(x, 1)\n",
        "    out = torch.cat((avg_pool, max_pool), 1)\n",
        "    out = self.out(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "def train(data_loader, model, optimizer, device):\n",
        "  model.train()\n",
        "\n",
        "  for data in data_loader:\n",
        "    reviews = data[\"review\"]\n",
        "    targets = data[\"target\"]\n",
        "\n",
        "    reviews = reviews.to(device, dtype=torch.long)\n",
        "    targets = reviews.to(device, dtype=torch.float)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(reviews)\n",
        "\n",
        "    loss = nn.BCEWithLogitsLoss()(\n",
        "        predictions,\n",
        "        targets.view(-1, 1)\n",
        "    )\n",
        "  \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def evaluate(data_loader, model, device):\n",
        "  final_predictions = []\n",
        "  final_targets = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      reviews = data[\"review\"]\n",
        "      targets = data[\"target\"]\n",
        "      reviews = reviews.to(device, dtype=torch.long)\n",
        "      targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "      predictions = model(reviews)\n",
        "\n",
        "      predictions = predictions.cpu().numpy().tolist()\n",
        "      targets = data[\"target\"].cpu().numpy().tolist()\n",
        "      final_predictions.extend(predictions)\n",
        "      final_targets.extend(targets)\n",
        "  \n",
        "  return final_predictions, final_targets\n",
        "\n",
        "def load_vectors(fname):\n",
        "  fin = io.open(\n",
        "      fname,\n",
        "      'r',\n",
        "      encoding='utf-8',\n",
        "      newline='\\n',\n",
        "      errors='ignore'\n",
        "  )\n",
        "  n, d = map(int, fin.readline().split())\n",
        "  \n",
        "  data = {}\n",
        "  \n",
        "  for line in fin:\n",
        "    tokens = line.rstrip().split(' ')\n",
        "    data[tokens[0]] = list(map(float, tokens[1:]))\n",
        "  \n",
        "  return data\n",
        "\n",
        "def creatre_embedding_matrix(word_index, embedding_dict):\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "  for word, i in word_index.items():\n",
        "    if word in embedding_dict:\n",
        "      embedding_matrix[i] = embedding_dict[word]\n",
        "  \n",
        "  return embedding_matrix\n",
        "\n",
        "def run(df, fold):\n",
        "  train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "  valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  print(\"fitting tokenizer\")\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "  tokenizer.fit_on_texts(df.review.values.tolist())\n",
        "\n",
        "  xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
        "  xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
        "\n",
        "  xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      xtrain, maxlen=128\n",
        "  )\n",
        "\n",
        "  xtest = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      xtest, maxlen=128\n",
        "  )\n",
        "\n",
        "  train_dataset = IMDBDataset(\n",
        "      reviews=xtrain,\n",
        "      targets=train_df.sentiment.values\n",
        "  )\n",
        "\n",
        "  train_data_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=16,\n",
        "      num_workers=2\n",
        "  )\n",
        "\n",
        "  valid_dataset = IMDBDataset(\n",
        "      reviews=xtest,\n",
        "      targets=valid_df.sentiment.values\n",
        "  )\n",
        "\n",
        "  valid_data_loader = torch.utils.data.DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size=16,\n",
        "      num_workers=1\n",
        "  )\n",
        "\n",
        "  print(\"loading embeddings\")\n",
        "\n",
        "  embedding_dict = load_vectors(\"/content/drive/My Drive/Project/imdb_review/crawl-300d-2M.vec\")\n",
        "  embedding_matrix = creatre_embedding_matrix(\n",
        "      tokenizer.word_index, embedding_dict\n",
        "  )\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  model = lstm.LSTM(embedding_matrix)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  print(\"training model\")\n",
        "  best_accuracy = 0\n",
        "  early_stopping_counter = 0\n",
        "\n",
        "  for epoch in range(10):\n",
        "    train(train_data_loader, model, optimizer, device)\n",
        "\n",
        "    outputs, targets = evaluate(\n",
        "        valid_data_loader, model, device\n",
        "    )\n",
        "\n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    print(\n",
        "        f\"FOLD: {fold}, Epoch: {epoch}, Accuracy_score: {accuracy}\"\n",
        "    )\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "      best_accuracy = accuracy\n",
        "    else:\n",
        "      early_stopping_counter += 1\n",
        "    \n",
        "    if early_stopping_counter > 2:\n",
        "      break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/Project/imdb_review/train_folds.csv\")\n",
        "  \n",
        "  run(df, fold=0)\n",
        "  run(df, fold=1)\n",
        "  run(df, fold=2)\n",
        "  run(df, fold=3)\n",
        "  run(df, fold=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-tS9kNsB5Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IMDBDataset:\n",
        "  def __init__(self, reviews, targets):\n",
        "    self.reviews = reviews\n",
        "    self.target = targets\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = self.reviews[item, :]\n",
        "    target = self.target[item]\n",
        "\n",
        "    return{\n",
        "        \"review\": torch.tensor(review, dtype=torch.long),\n",
        "        \"target\": torch.tensor(target, dt)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ijoI9hCb1rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}