{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk-tweet-trax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkECp/A/8cGUXCcWSP9izs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiabe/coding-practice/blob/master/nltk_tweet_trax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L1d32uJlh6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8fa919c4-7a85-4d18-8eeb-c8821bc14809"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ExvfPgmtz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U trax\n",
        "import trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmzWHOQTmJBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import twitter_samples \n",
        "from trax import fastmath\n",
        "np = fastmath.numpy\n",
        "random = fastmath.random\n",
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \n",
        "\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "val_pos = all_positive_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "val_neg = all_negative_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "val_x = val_pos + val_neg\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "val_y = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPsujKSkoaEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "bb42d5e9-ef5f-4a8e-f1b9-1834afd6c3bc"
      },
      "source": [
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stopwords_english = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet) # remove \"$\"\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet) # remove \"RT\"\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet) # remove \"https\"\n",
        "  tweet = re.sub(r'#', '', tweet) # remove \"#\"\n",
        "    \n",
        "  tokenizer = TweetTokenizer(\n",
        "      preserve_case=False,\n",
        "      strip_handles=True,\n",
        "      reduce_len=True\n",
        "  )\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    \n",
        "  tweets_clean = []\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)\n",
        "      tweets_clean.append(stem_word)\n",
        "  return tweets_clean\n",
        "\n",
        "# test\n",
        "print(train_pos[0])\n",
        "print(process_tweet(train_pos[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVqEYPCLsLvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4204208c-3e36-4b01-afdc-633ce55e49f1"
      },
      "source": [
        "Vocab = {\n",
        "    '__PAD__': 0,\n",
        "    '__</e>__': 1,\n",
        "    '__UNK__': 2\n",
        "}\n",
        "#print(len(Vocab))\n",
        "\n",
        "for tweet in train_x:\n",
        "  processed_tweet = process_tweet(tweet)\n",
        "  for word in processed_tweet:\n",
        "    if word not in Vocab:\n",
        "      Vocab[word] = len(Vocab)\n",
        "print(len(Vocab))\n",
        "#display(Vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB3QOAnSsL1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ca34e5da-b4b0-4a58-c894-3a537a957002"
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "  word_l = process_tweet(tweet)\n",
        "  if verbose:\n",
        "    print(\"list of word from the processed tweet:\")\n",
        "    print(word_l)\n",
        "\n",
        "  tensor_l = []\n",
        "  unk_ID = vocab_dict[unk_token]\n",
        "  if verbose:\n",
        "    print(f\"the unique integer ID for the unk_token is {unk_ID}\")\n",
        "\n",
        "  for word in word_l:\n",
        "    word_ID = vocab_dict[word] if word in vocab_dict.keys() else unk_ID\n",
        "    tensor_l.append(word_ID)\n",
        "  return tensor_l\n",
        "\n",
        "# test\n",
        "print(train_pos[0])\n",
        "print(process_tweet(train_pos[0]))  \n",
        "print(tweet_to_tensor(train_pos[0], Vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
            "[3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBfE41opsL5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rnd\n",
        "\n",
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "  assert batch_size % 2 == 0\n",
        "  n_to_take = batch_size // 2\n",
        "  \n",
        "  pos_index = 0\n",
        "  neg_index = 0\n",
        "  len_data_pos = len(train_pos)\n",
        "  len_data_neg = len(train_neg)\n",
        "  \n",
        "  pos_index_lines = [*range(len_data_pos)]\n",
        "  neg_index_lines = [*range(len_data_neg)]\n",
        "\n",
        "  if shuffle:\n",
        "    rnd.shuffle(pos_index_lines)\n",
        "    rnd.shuffle(neg_index_lines)\n",
        "\n",
        "  stop = False\n",
        "\n",
        "  while not stop:\n",
        "    batch = []\n",
        "    for i in range(n_to_take):\n",
        "      if pos_index >= len_data_pos:\n",
        "        if not loop:\n",
        "          stop = True;\n",
        "          break;\n",
        "        pos_index = 0\n",
        "        if shuffle:\n",
        "          rnd.shuffle(pos_index_lines)\n",
        "      tweet =  train_pos[pos_index_lines[pos_index]]\n",
        "      tensor = tweet_to_tensor(tweet, Vocab)\n",
        "      batch.append(tensor)\n",
        "      pos_index = pos_index + 1\n",
        "\n",
        "    for i in range(n_to_take):\n",
        "      if neg_index >= len_data_neg:\n",
        "        if not loop:\n",
        "          stop = True;\n",
        "          break;\n",
        "        neg_index = 0\n",
        "        if shuffle:\n",
        "          rnd.shuffle(neg_index_lines)\n",
        "      tweet = train_neg[neg_index_lines[neg_index]]\n",
        "      tensor = tweet_to_tensor(tweet, Vocab)\n",
        "      batch.append(tensor)\n",
        "      neg_index = neg_index + 1\n",
        "    \n",
        "    if stop:\n",
        "      break;\n",
        "\n",
        "    pos_index += n_to_take\n",
        "    neg_index += n_to_take\n",
        "    max_len = max(len(t) for t in batch)\n",
        "\n",
        "    tensor_pad_l = []\n",
        "    for tensor in batch:\n",
        "      n_pad = max_len - len(tensor)\n",
        "      pad_l = [0] * n_pad\n",
        "      tensor_pad = tensor + pad_l\n",
        "      tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "    inputs = np.array(tensor_pad_l)\n",
        "\n",
        "    target_pos = [1] * n_to_take\n",
        "    target_neg = [0] * n_to_take\n",
        "    target_l = target_pos + target_neg\n",
        "    targets = np.array(target_l)\n",
        "\n",
        "    example_weights = np.ones_like(targets)\n",
        "\n",
        "    yield inputs, targets, example_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m0L7B3FoaBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "552d81f7-42d3-4a3c-8400-34a38a809f50"
      },
      "source": [
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs: [[ 238 4507 1584  453    9    0    0    0    0    0    0    0    0    0]\n",
            " [  10 4255  100  366  610  345  429  790  610    9  263  343   98 1799]\n",
            " [  73  459  460 3761    0    0    0    0    0    0    0    0    0    0]\n",
            " [ 363  136 4105 6282  269 6283   63 5749    0    0    0    0    0    0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHZ3muGgoaHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e615e28b-b560-4329-b16b-e354dc8e134c"
      },
      "source": [
        "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
        "  embed_layer = trax.layers.Embedding(\n",
        "      vocab_size=vocab_size,\n",
        "      d_feature=embedding_dim\n",
        "  )\n",
        "  mean_layer = trax.layers.Mean(axis=1)\n",
        "  dense_output_layer = trax.layers.Dense(n_units=output_dim)\n",
        "  log_softmax = trax.layers.LogSoftmax()\n",
        "\n",
        "  model = trax.layers.Serial(\n",
        "      embed_layer,\n",
        "      mean_layer,\n",
        "      dense_output_layer,\n",
        "      log_softmax\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "model = classifier()\n",
        "display(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_9092_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az--iPGNoaJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "\n",
        "batch_size = 16\n",
        "rnd.seed(271)\n",
        "\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.01),\n",
        "    n_steps_per_checkpoint=10,\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")\n",
        "\n",
        "model = classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvzNHusMoaMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "003d87f6-5d1c-4c47-9c13-217b1f4cb36b"
      },
      "source": [
        "output_dir = '~/model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV3tr3lDoaOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_task, # The evaluation task\n",
        "                                output_dir) # The output directory\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "### END CODE HERE ###\n",
        "\n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkI9pqVxJwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "2b6c1d6a-0f3e-405a-e679-0d8ad07bf188"
      },
      "source": [
        "training_loop = train_model(\n",
        "    classifier=model,\n",
        "    train_task=train_task,\n",
        "    eval_task=eval_task,\n",
        "    n_steps=100,\n",
        "    output_dir=output_dir_expand\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c24c6222fab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0meval_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir_expand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-32-deaa29df631b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, train_task, eval_task, n_steps, output_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0mtrain_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The training task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                 \u001b[0meval_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The evaluation task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                 output_dir) # The output directory\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtraining_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/supervised/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tasks, eval_model, eval_tasks, output_dir, checkpoint_at, eval_at, n_devices, random_seed)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0meval_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_tasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Multitask training not supported yet.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0meval_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Multitask training not supported yet."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt2ftBz2xJ1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCe1dylZxJ5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjok3BrzxJ72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unxR6blwxJ-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}